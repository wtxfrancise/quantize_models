[**ğŸ‡¨ğŸ‡³ä¸­æ–‡**](./README.md) | [**English**](./README_EN.md) |

# quantize_models
This code repository aims to collect various implementation methods for the quantization of large models

# é‡åŒ–æ–¹æ¡ˆ

## 1.llama
<p>å…·ä½“çš„æ¨¡å‹è¯´æ˜å’Œè§£é‡Šå¯ä»¥å‚è€ƒå…¶[å®˜ç½‘](https://github.com/facebookresearch/llama) å’Œ [llama.cpp](https://github.com/ggerganov/llama.cpp)</p>
è¿™é‡Œå°±å…¶é‡åŒ–ç­–ç•¥è¿›è¡Œæ€»ç»“:
<p>1.1.æ ¹æ®éœ€è¦ä¸‹è½½æŒ‡å®šçš„æ¨¡å‹,ä½ å¯ä»¥åœ¨huggingfaceä¸Šé€‰æ‹©ä½ æƒ³è¦çš„æ¨¡å‹å¤§å°ç‰ˆæœ¬ï¼Œå…¶ä¸­æœ‰llama-7b,llama-13b,llama-30b,llama-65bç­‰ç‰ˆæœ¬</p>
<p>1.2.ä½¿ç”¨gitå¯¹äºéœ€è¦çš„æ¨¡å‹æ–‡ä»¶ä¸‹è½½ï¼Œå¹¶ä¸”æ”¾åœ¨æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹ï¼Œå› ä¸ºæ˜¯å¤šä¸ªæ–‡ä»¶ï¼Œæ‰€ä»¥å¯ä»¥ä½¿ç”¨å¦‚ä¸‹çš„gitå‘½ä»¤,è¿›è¡Œæ•´ä¸ªæ–‡ä»¶å¤¹çš„ä¸‹è½½,å…¶ä¸­[]ä¸­çš„å†…å®¹ä¸ºä½ çš„ä¸‹è½½çš„ç›®æ ‡ç›®å½•ï¼Œå¦‚huggyllama/llama-7b</p>

```
git clone https://huggingface.co/[ä½ éœ€è¦ä¸‹è½½çš„ç›®å½•].git
```
1.3.ä½¿ç”¨llama.cppçš„é‡åŒ–ç­–ç•¥è¿›è¡Œé‡åŒ–ï¼Œå…·ä½“çš„å¯ä»¥å‚è€ƒ[llama.cpp](https://github.com/ggerganov/llama.cpp),ä¸»è¦æ­¥éª¤æ˜¯,

1.3.1 å…‹éš†å’Œç¼–è¯‘llama.cpp,ä½†æ˜¯é’ˆå¯¹ä¸åŒçš„å¹³å°ä½¿ç”¨çš„ç¼–è¯‘æ–¹æ³•ç¨å¾®æœ‰ä¸€ç‚¹åŒºåˆ«
- ä½¿ç”¨makeè¿›è¡Œç¼–è¯‘
Linuxå’ŒMacOs,å› ä¸ºcçš„å·¥å…·é“¾æ¯”è¾ƒé½å…¨ï¼Œcd åˆ°llama.cpp,makeå³å¯;ä½†æ˜¯åœ¨windowsä¸Šé¢ï¼Œå¯ä»¥é€šè¿‡ä¸‹è½½è¿è¡Œ[w64devkit](https://github.com/skeeto/w64devkit/releases)ï¼Œå¸¸å¸¸å› ä¸ºè¯­è¨€åº“çš„å†²çªå¯¼è‡´ç¼–è¯‘å¤±è´¥ï¼Œæ‰€ä»¥å»ºè®®åœ¨windowsä¸Šé¢ç›´æ¥ä½¿ç”¨cmake
- ä½¿ç”¨cmakeè¿›è¡Œç¼–è¯‘ï¼Œå¦‚æœæ²¡æœ‰cmakeå·¥å…·,å¯ä»¥ç›´æ¥é€šè¿‡pip install cmakeå®‰è£…,å®‰è£…å®Œæˆå,cdåˆ°llama.cppæ–‡ä»¶å¤¹ä¸‹åï¼Œå…·ä½“å‘½ä»¤å¦‚ä¸‹,
```shell
mkdir build
cd build
cmake ..
cmake --build . --config Release
```
è¿™ä¸ªæ—¶å€™ç”Ÿæˆä¸€ä¸ªbuildæ–‡ä»¶å¤¹ï¼Œé‡Œé¢æœ‰quantize,main ç­‰æ–‡ä»¶

1.3.2 é‡åŒ–æ¨¡å‹
- å®‰è£…llama.cppçš„ä¾èµ–åº“
```shell
python3 -m pip install -r requirements.txt
```
- å°†ä¸Šé¢æ­¥éª¤2ä¸­ä¸‹è½½çš„llamaæ¨¡å‹è½¬æ¢ä¸ºFP16çš„æ ¼å¼.ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤,[]å†…éœ€è¦ä¿®æ”¹ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
```shell
python3 convert.py [your model path]
```
- å°†ç”Ÿæˆçš„FP16æ ¼å¼çš„æ¨¡å‹æ–‡ä»¶è¿›è¡Œ4-bitsçš„é‡åŒ–ï¼Œä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ï¼Œ

å¦‚æœæ˜¯Linux æˆ–æ˜¯ MacOS ç¬¬ä¸€ä¸ª[]å†…è¾“å…¥ï¼Œä¾‹å¦‚ï¼š./models/7B/ggml-model-f16.bin ; ç¬¬äºŒä¸ª[] å†…è¾“å…¥ä½ ç”Ÿæˆæ¨¡å‹çš„ç›®æ ‡è·¯å¾„,ä¾‹å¦‚:./models/7B/ggml-model-q4_0.bin
```shell
./quantize [your f16 file path] [your q4 model path] 2
```
å¦‚æœæ˜¯windows,ä¸¤ä¸ª[]ä¸­çš„å†…å®¹åŒä¸Š
```shell
build\bin\Release\quantize [your f16 file path] [your q4 model path] 2
```

1.4.è¿è¡Œä½ çš„é‡åŒ–æ¨¡å‹

1.4.1 åœ¨å‘½ä»¤è¡Œä¸­æ‰§è¡Œ

- åœ¨Linux æˆ–æ˜¯ MacOS è¿è¡Œ:
```shell
./main -m [your q4 model path] -n 128
```
- åœ¨windows è¿è¡Œ:
```shell
\build\bin\Release\main -m [your q4 model path] -n 128
```
1.4.2 ä½¿ç”¨llama_cppåº“æ¥è½½å…¥æ¨¡å‹,å¦‚ä½¿ç”¨å¦‚ä¸‹pythonä»£ç :

å…ˆä½¿ç”¨pip å®‰è£…llama_cppåº“
```shell
pip install llama_cpp
```
```python
from llama_cpp import Llama
path = "your q4 model path"
llm = Llama(model_path=path)
output = llm("Q: what is your name? A: ", max_tokens=32, stop=["Q:", "\n"], echo=True)
print(output)
```

-----------

## 2.Chimera
å› ä¸º[Chimera](https://github.com/FreedomIntelligence/LLMZoo)æ˜¯åŸºäºllamaæ¡†æ¶æ­å»ºçš„è®­ç»ƒæ¨¡å‹ï¼Œæ‰€ä»¥å…¶é‡åŒ–æ–¹æ¡ˆå¯ä»¥å‚è€ƒllamaçš„quantizationçš„æ–¹æ¡ˆï¼Œå…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š

2.1.1ä¸‹è½½Chimeraçš„å¢é‡åŒ–æ¨¡å‹ï¼Œå¯¹åº”çš„é“¾æ¥å¯ä»¥åœ¨LLMZooä¸­è¿›è¡Œ[ä¸‹è½½](https://github.com/FreedomIntelligence/LLMZoo)ï¼Œ

å¦‚ï¼Œæˆ‘éœ€è¦ä¸‹è½½7bçš„å¢é‡æ¨¡å‹ï¼Œå¯ä»¥åœ¨æŒ‡å®šçš„æ–‡ä»¶å¤¹ä¸‹ï¼Œæ‰“å¼€gitæ§åˆ¶å°ï¼Œè¾“å…¥å¦‚ä¸‹æŒ‡ä»¤è¿›è¡Œæ¨¡å‹ä¸‹è½½ï¼š 
```git
git clone https://huggingface.co/FreedomIntelligence/chimera-chat-7b-delta.git
```
2.1.2.ä¸‹è½½llamaæ¨¡å‹ï¼Œå…·ä½“ä¸‹è½½æ–¹å¼è§ä¸Šæ–‡llamaä¸­ç¬¬äºŒæ¡ï¼Œ

2.2.ä½¿ç”¨[LLMZoo](https://github.com/FreedomIntelligence/LLMZoo)é¡¹ç›®ä¸­çš„[apply_delta.py](https://github.com/FreedomIntelligence/LLMZoo/tree/main/tools)å‚æ•°åˆå¹¶å‡½æ•°ï¼Œè¿›è¡Œå‚æ•°åˆå¹¶,
å…·ä½“æŒ‡ä»¤å¦‚ä¸‹ï¼š
```shell
python tools/apply_delta.py  --base [llamaæ¨¡å‹çš„ä¸‹è½½è·¯å¾„]  --target [ç”Ÿæˆçš„Chimeraç›®æ ‡è·¯å¾„]  --delta [Chimeraçš„ä¸‹è½½è·¯å¾„]
```
2.3.å¯ä»¥ä½¿ç”¨ä¸Šæ–‡llamaä¸­çš„é‡åŒ–ç­–ç•¥,å…·ä½“çš„å¯ä»¥å‚è€ƒ[llama.cpp](https://github.com/ggerganov/llama.cpp),ä¸»è¦æ­¥éª¤æ˜¯,
2.3.1 å…‹éš†å’Œç¼–è¯‘llama.cpp,ä½†æ˜¯é’ˆå¯¹ä¸åŒçš„å¹³å°ä½¿ç”¨çš„ç¼–è¯‘æ–¹æ³•ç¨å¾®æœ‰ä¸€ç‚¹åŒºåˆ«
- ä½¿ç”¨makeè¿›è¡Œç¼–è¯‘
Linuxå’ŒMacOs,å› ä¸ºcçš„å·¥å…·é“¾æ¯”è¾ƒé½å…¨ï¼Œcd åˆ°llama.cpp,makeå³å¯;ä½†æ˜¯åœ¨windowsä¸Šé¢ï¼Œå¯ä»¥é€šè¿‡ä¸‹è½½è¿è¡Œ[w64devkit](https://github.com/skeeto/w64devkit/releases)ï¼Œå¸¸å¸¸å› ä¸ºè¯­è¨€åº“çš„å†²çªå¯¼è‡´ç¼–è¯‘å¤±è´¥ï¼Œæ‰€ä»¥å»ºè®®åœ¨windowsä¸Šé¢ç›´æ¥ä½¿ç”¨cmake
- ä½¿ç”¨cmakeè¿›è¡Œç¼–è¯‘ï¼Œå¦‚æœæ²¡æœ‰cmakeå·¥å…·,å¯ä»¥ç›´æ¥é€šè¿‡pip install cmakeå®‰è£…,å®‰è£…å®Œæˆå,cdåˆ°llama.cppæ–‡ä»¶å¤¹ä¸‹åï¼Œå…·ä½“å‘½ä»¤å¦‚ä¸‹,
```shell
mkdir build
cd build
cmake ..
cmake --build . --config Release
```
è¿™ä¸ªæ—¶å€™ç”Ÿæˆä¸€ä¸ªbuildæ–‡ä»¶å¤¹ï¼Œé‡Œé¢æœ‰quantize,main ç­‰æ–‡ä»¶
- ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨Chimeraæ–‡ä»¶å¤¹å·²ç»ç”Ÿæˆå¥½çš„buildæ–‡ä»¶ç›®å½•è¿›è¡Œç¼–è¯‘
2.3.2 é‡åŒ–æ¨¡å‹
- å®‰è£…llama.cppçš„ä¾èµ–åº“
```shell
python3 -m pip install -r requirements.txt
```
- å°†ä¸Šé¢æ­¥éª¤2ä¸­ä¸‹è½½çš„llamaæ¨¡å‹è½¬æ¢ä¸ºFP16çš„æ ¼å¼.ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤,[]å†…éœ€è¦ä¿®æ”¹ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
```shell
python3 convert.py [your model path]
```
- å°†ç”Ÿæˆçš„FP16æ ¼å¼çš„æ¨¡å‹æ–‡ä»¶è¿›è¡Œ4-bitsçš„é‡åŒ–ï¼Œä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ï¼Œ

å¦‚æœæ˜¯Linux æˆ–æ˜¯ MacOS ç¬¬ä¸€ä¸ª[]å†…è¾“å…¥ï¼Œä¾‹å¦‚ï¼š./models/7B/ggml-model-f16.bin ; ç¬¬äºŒä¸ª[] å†…è¾“å…¥ä½ ç”Ÿæˆæ¨¡å‹çš„ç›®æ ‡è·¯å¾„,ä¾‹å¦‚:./models/7B/ggml-model-q4_0.bin
```shell
./quantize [your f16 file path] [your q4 model path] 2
```
å¦‚æœæ˜¯windows,ä¸¤ä¸ª[]ä¸­çš„å†…å®¹åŒä¸Š
```shell
build\bin\Release\quantize [your f16 file path] [your q4 model path] 2
```

2.4.è¿è¡Œä½ çš„é‡åŒ–æ¨¡å‹

2.4.1 åœ¨å‘½ä»¤è¡Œä¸­æ‰§è¡Œ

- åœ¨Linux æˆ–æ˜¯ MacOS è¿è¡Œ:
```shell
./main -m [your q4 model path] -n 128
```
- åœ¨windows è¿è¡Œ:
```shell
\build\bin\Release\main -m [your q4 model path] -n 128
```
2.4.2 ä½¿ç”¨llama_cppåº“æ¥è½½å…¥æ¨¡å‹,å¦‚ä½¿ç”¨å¦‚ä¸‹pythonä»£ç :
å…ˆä½¿ç”¨pip å®‰è£…llama_cppåº“
```shell
pip install llama_cpp
```

```python
from llama_cpp import Llama
path = "your q4 model path"
llm = Llama(model_path=path)
output = llm("Q: what is your name? A: ", max_tokens=32, stop=["Q:", "\n"], echo=True)
print(output)
```

2.4.3 å†…å­˜/ç¡¬ç›˜éœ€æ±‚
æ‚¨éœ€è¦è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´æ¥ä¿å­˜å®ƒä»¬ï¼Œå¹¶éœ€è¦è¶³å¤Ÿçš„RAMæ¥åŠ è½½å®ƒä»¬ï¼Œå†…å­˜å’Œç£ç›˜éœ€æ±‚æ˜¯ç›¸åŒçš„ï¼Œä¸‹é¢æ˜¯å…·ä½“çš„éœ€æ±‚ã€‚

| Model            | Original size | Quantized size (4-bit) |
|------------------|---------------|------------------------|
| chimera-chat-7B  | 12.5 GB       | 3.9GB                  |
| chimera-inst-chat-7B | 12.5 GB       | 3.9GB                    |


2.5 é‡åŒ–æ–‡ä»¶ä¸‹è½½ï¼Œå¯ä»¥é€šè¿‡ç™¾åº¦ç½‘ç›˜ï¼š
é“¾æ¥ï¼šhttps://pan.baidu.com/s/1mKPiPvmO1BeZPhkwH6F1QQ  æå–ç ï¼š6fzy 


-----------
## 3.bloomz
3.1 ä¸‹è½½bloomzæ¨¡å‹ï¼Œå…·ä½“å¯ä»¥ç‚¹å‡»[é“¾æ¥åœ°å€](https://github.com/NouamaneTazi/bloomz.cpp)è¿›è¡ŒæŸ¥çœ‹

3.2 ç¼–è¯‘

3.2.1 è‡ªè¡Œç¼–è¯‘ï¼Œå¯ä»¥ä½¿ç”¨makeå·¥å…·è¿›è¡Œç¼–è¯‘, å…·ä½“æŒ‡ä»¤ä¸º:
```shell
git clone https://github.com/NouamaneTazi/bloomz.cpp && cd bloomz.cpp
make
```
3.2.2 ç›´æ¥ä½¿ç”¨å·²ç»åœ¨Linuxç¼–è¯‘å¥½çš„æ–‡ä»¶ï¼Œæ–‡ä»¶è§Phoenix/Linux æ–‡ä»¶å¤¹ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨

3.3 ä½¿ç”¨bloomçš„é‡åŒ–æ–¹æ¡ˆ

3.3.1 å¯ä»¥æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤,å°†Phoenixè½¬æ¢ä¸ºggmlæ ¼å¼ï¼Œä½¿ç”¨å¦‚ä¸‹æŒ‡ä»¤
```shell

python3 convert-hf-to-ggml.py [ä½ ä¸‹è½½çš„æ¨¡å‹è·¯å¾„] [ä½ å¸Œæœ›å­˜å‚¨ggmlæ ¼å¼çš„è·¯å¾„] 

```
3.3.2 å¾—åˆ°æŒ‡å®šçš„ggmlæ–‡ä»¶åï¼Œè¿›è¡Œ4-bitsé‡åŒ–ï¼Œå…·ä½“æŒ‡ä»¤å¦‚ä¸‹:

```shell
./quantize [ggmlæ ¼å¼è·¯å¾„] [é‡åŒ–åçš„æ ¼å¼è·¯å¾„] 2

å¦‚ï¼š./quantize ./models/ggml-model-bloomz-7b1-f16.bin ./models/ggml-model-bloomz-7b1-f16-q4_0.bin 2
```

3.4 é€šè¿‡ç¼–è¯‘çš„mainæ–‡ä»¶ï¼Œè¿›è¡Œé‡åŒ–åçš„ç»“æœéªŒè¯:
```
./main -m models/ggml-model-bloomz-7b1-f16-q4_0.bin  -p 'Translate "Hi, how are you?" in French:' -t 8 -n 256
```
3.5 å¯¹åº”çš„é‡åŒ–æ¨¡å‹å¯ä»¥[ç‚¹å‡»è¿™é‡Œä¸‹è½½](https://huggingface.co/Wauplin/bloomz-7b1.cpp/tree/main)

-----------
## 4.Phoenix
4.1 ä¸‹è½½Phoenixæ¨¡å‹ï¼Œå…·ä½“å¯ä»¥ç‚¹å‡»[é“¾æ¥åœ°å€](https://github.com/FreedomIntelligence/LLMZoo)è¿›è¡ŒæŸ¥çœ‹

4.2 å› ä¸ºPhoenixæ˜¯åŸºäºbloomæ–¹æ¡ˆï¼Œæ‰€ä»¥å¯ä»¥è¿ç§»å…¶é‡åŒ–æ–¹æ¡ˆï¼Œå…·ä½“çš„å¯ä»¥[ç‚¹å‡»è¿™é‡Œ](https://github.com/NouamaneTazi/bloomz.cpp)è¿›è¡ŒæŸ¥çœ‹ï¼Œ

4.2.1 è‡ªè¡Œç¼–è¯‘ï¼Œå¯ä»¥ä½¿ç”¨makeå·¥å…·è¿›è¡Œç¼–è¯‘, å…·ä½“æŒ‡ä»¤ä¸º:
```shell
git clone https://github.com/NouamaneTazi/bloomz.cpp && cd bloomz.cpp
make
```
4.2.2 ç›´æ¥ä½¿ç”¨å·²ç»åœ¨Linuxç¼–è¯‘å¥½çš„æ–‡ä»¶ï¼Œæ–‡ä»¶è§Phoenix/Linux æ–‡ä»¶å¤¹ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨

4.3 ä½¿ç”¨bloomçš„é‡åŒ–æ–¹æ¡ˆ

4.3.1 å¯ä»¥æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤,å°†Phoenixè½¬æ¢ä¸ºggmlæ ¼å¼ï¼Œä½¿ç”¨å¦‚ä¸‹æŒ‡ä»¤
```shell
python3 convert-hf-to-ggml.py [ä½ ä¸‹è½½çš„æ¨¡å‹è·¯å¾„] [ä½ å¸Œæœ›å­˜å‚¨ggmlæ ¼å¼çš„è·¯å¾„] 

å¦‚ï¼špython3 Phoenix\convert-hf-to-ggml.py .\LLMModel\phoenix-chat-7b ./models
```

4.3.2 å¾—åˆ°æŒ‡å®šçš„ggmlæ–‡ä»¶åï¼Œè¿›è¡Œ4-bitsé‡åŒ–ï¼Œå…·ä½“æŒ‡ä»¤å¦‚ä¸‹:

```shell
./quantize [ggmlæ ¼å¼è·¯å¾„] [é‡åŒ–åçš„æ ¼å¼è·¯å¾„] 2

å¦‚ï¼š./quantize ./models/ggml-model-bloomz-7b1-f16.bin ./models/ggml-model-bloomz-7b1-f16-q4_0.bin 2
```

4.3.4 å†…å­˜/ç¡¬ç›˜éœ€æ±‚
æ‚¨éœ€è¦è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´æ¥ä¿å­˜å®ƒä»¬ï¼Œå¹¶éœ€è¦è¶³å¤Ÿçš„RAMæ¥åŠ è½½å®ƒä»¬ï¼Œå†…å­˜å’Œç£ç›˜éœ€æ±‚æ˜¯ç›¸åŒçš„ï¼Œä¸‹é¢æ˜¯å…·ä½“çš„éœ€æ±‚ã€‚

| Model            | Original size | Quantized size (4-bit) |
|------------------|---------------|------------------------|
| phoenix-chat-7b  | 15 GB         | 4.7GB                 |



4.4 é€šè¿‡ç¼–è¯‘çš„mainæ–‡ä»¶ï¼Œè¿›è¡Œé‡åŒ–åçš„ç»“æœéªŒè¯:
```
./main -m models/ggml-model-bloomz-7b1-f16-q4_0.bin  -p 'Translate "Hi, how are you?" in French:' -t 8 -n 256
```

4.5 å¯ä»¥åœ¨ä¸‹é¢çš„ç™¾åº¦ç½‘ç›˜è¿›è¡Œé‡åŒ–æ¨¡å‹çš„ä¸‹è½½ï¼Œåˆ†äº«åœ°å€ä¸ºï¼š
é“¾æ¥ï¼šhttps://pan.baidu.com/s/1iyuREwapZLttfT6VhuKqhQ  æå–ç ï¼šs0us 


-----------
## 5.ChatGLM
todo

----------
## 6.Falcon
todo

